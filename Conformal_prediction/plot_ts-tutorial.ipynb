{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tutorial for time series\n\nIn this tutorial we describe how to use\n:class:`~mapie.time_series_regression.MapieTimeSeriesRegressor`\nto estimate prediction intervals associated with time series forecast.\n\nHere, we use the Victoria electricity demand dataset used in the book\n\"Forecasting: Principles and Practice\" by R. J. Hyndman and G. Athanasopoulos.\nThe electricity demand features daily and weekly seasonalities and is impacted\nby the temperature, considered here as a exogeneous variable.\n\nBefore estimating prediction intervals with MAPIE, we optimize the base model,\nhere a Random Forest model. The hyper-parameters are\noptimized with a :class:`~sklearn.model_selection.RandomizedSearchCV` using a\nsequential :class:`~sklearn.model_selection.TimeSeriesSplit` cross validation,\nin which the training set is prior to the validation set.\n\nOnce the base model is optimized, we can use\n:class:`~MapieTimeSeriesRegressor` to estimate\nthe prediction intervals associated with one-step ahead forecasts through\nthe EnbPI method [1].\n\nAs its parent class :class:`~MapieRegressor`,\n:class:`~MapieTimeSeriesRegressor` has two main arguments : \"cv\", and \"method\".\nIn order to implement EnbPI, \"method\" must be set to \"enbpi\" (the default\nvalue) while \"cv\" must be set to the :class:`~mapie.subsample.BlockBootstrap`\nclass that block bootstraps the training set.\nThis sampling method is used in [1] instead of the traditional bootstrap\nstrategy as it is more suited for time series data.\n\nThe EnbPI method allows you update the residuals during the prediction,\neach time new observations are available so that the deterioration of\npredictions, or the increase of noise level, can be dynamically taken into\naccount. It can be done with :class:`~MapieTimeSeriesRegressor` through\nthe ``partial_fit`` class method called at every step.\n\n[1] Chen Xu and Yao Xie.\n\u201cConformal Prediction Interval for Dynamic Time-Series.\u201d\nInternational Conference on Machine Learning (ICML, 2021).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt\nfrom scipy.stats import randint\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n\nfrom mapie.metrics import (regression_coverage_score,\n                           regression_mean_width_score)\nfrom mapie.subsample import BlockBootstrap\nfrom mapie.time_series_regression import MapieTimeSeriesRegressor\n\nwarnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load input data and dataset preparation\n\nThe Victoria electricity demand dataset can be downloaded directly on the\nMAPIE github repository. It consists in hourly electricity demand (in GW)\nof the Victoria state in Australia together with the temperature\n(in Celsius degrees). We extract temporal features out of the date and hour.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_test_steps = 24 * 7\n\nurl_file = (\n    \"https://raw.githubusercontent.com/scikit-learn-contrib/MAPIE/master/\"\n    \"examples/data/demand_temperature.csv\"\n)\ndemand_df = pd.read_csv(\n    url_file, parse_dates=True, index_col=0\n)\ndemand_df[\"Date\"] = pd.to_datetime(demand_df.index)\ndemand_df[\"Weekofyear\"] = demand_df.Date.dt.isocalendar().week.astype(\"int64\")\ndemand_df[\"Weekday\"] = demand_df.Date.dt.isocalendar().day.astype(\"int64\")\ndemand_df[\"Hour\"] = demand_df.index.hour\nn_lags = 5\nfor hour in range(1, n_lags):\n    demand_df[f\"Lag_{hour}\"] = demand_df[\"Demand\"].shift(hour)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now introduce a brutal changepoint in the test set by decreasing the\nelectricity demand by 2 GW on February 22.\nIt aims at simulating an effect, such as blackout or lockdown due to a\npandemic, that was not taken into account by the model during its training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "demand_df.Demand.iloc[-int(num_test_steps/2):] -= 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last week of the dataset is considered as test set, the remaining data\nis used as training set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "demand_train = demand_df.iloc[:-num_test_steps, :].copy()\ndemand_test = demand_df.iloc[-num_test_steps:, :].copy()\nfeatures = [\"Weekofyear\", \"Weekday\", \"Hour\", \"Temperature\"]\nfeatures += [f\"Lag_{hour}\" for hour in range(1, n_lags)]\n\nX_train = demand_train.loc[\n    ~np.any(demand_train[features].isnull(), axis=1), features\n]\ny_train = demand_train.loc[X_train.index, \"Demand\"]\nX_test = demand_test.loc[:, features]\ny_test = demand_test[\"Demand\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now visualize the training and test sets with the changepoint.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 5))\nplt.plot(y_train)\nplt.plot(y_test)\nplt.ylabel(\"Hourly demand (GW)\")\nplt.legend([\"Training data\", \"Test data\"])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Optimize the base estimator\n\nBefore estimating the prediction intervals with MAPIE, let's optimize the\nbase model, here a :class:`~RandomForestRegressor` through a\n:class:`~RandomizedSearchCV` with a temporal cross-validation strategy.\nFor the sake of computational time, the best parameters are already tuned.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_params_fit_not_done = False\nif model_params_fit_not_done:\n    # CV parameter search\n    n_iter = 100\n    n_splits = 5\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    random_state = 59\n    rf_model = RandomForestRegressor(random_state=random_state)\n    rf_params = {\"max_depth\": randint(2, 30), \"n_estimators\": randint(10, 100)}\n    cv_obj = RandomizedSearchCV(\n        rf_model,\n        param_distributions=rf_params,\n        n_iter=n_iter,\n        cv=tscv,\n        scoring=\"neg_root_mean_squared_error\",\n        random_state=random_state,\n        verbose=0,\n        n_jobs=-1,\n    )\n    cv_obj.fit(X_train, y_train)\n    model = cv_obj.best_estimator_\nelse:\n    # Model: Random Forest previously optimized with a cross-validation\n    model = RandomForestRegressor(\n        max_depth=10, n_estimators=50, random_state=59)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Estimate prediction intervals on the test set\n\nWe now use :class:`~MapieTimeSeriesRegressor` to build prediction intervals\nassociated with one-step ahead forecasts. As explained in the introduction,\nwe use the EnbPI method [1].\n\nEstimating prediction intervals can be possible in two ways:\n\n- with a regular ``.fit`` and ``.predict`` process, limiting the use of\n  trainining set residuals to build prediction intervals\n\n- using ``.partial_fit`` in addition to ``.fit`` and ``.predict`` allowing\n  MAPIE to use new residuals from the test points as new data are becoming\n  available.\n\nThe latter method is particularly useful to adjust prediction intervals to\nsudden change points on test sets that have not been seen by the model\nduring training.\n\nFollowing [1], we use the :class:`~BlockBootstrap` sampling\nmethod instead of the traditional bootstrap strategy for training the model\nsince the former is more suited for time series data.\nHere, we choose to perform 100 resamplings with blocks of 48 points.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alpha = 0.05\ngap = 1\ncv_mapiets = BlockBootstrap(\n    n_resamplings=100, length=48, overlapping=True, random_state=59\n)\nmapie_enbpi = MapieTimeSeriesRegressor(\n    model, method=\"enbpi\", cv=cv_mapiets, agg_function=\"mean\", n_jobs=-1\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by estimating prediction intervals without partial fit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mapie_enbpi = mapie_enbpi.fit(X_train, y_train)\ny_pred_npfit, y_pis_npfit = mapie_enbpi.predict(\n    X_test, alpha=alpha, ensemble=True, optimize_beta=True\n)\ncoverage_npfit = regression_coverage_score(\n    y_test, y_pis_npfit[:, 0, 0], y_pis_npfit[:, 1, 0]\n)\nwidth_npfit = regression_mean_width_score(\n    y_pis_npfit[:, 0, 0], y_pis_npfit[:, 1, 0]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now estimate prediction intervals with partial fit. As discussed\npreviously, the update of the residuals and the one-step ahead predictions\nare performed sequentially in a loop.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mapie_enbpi = mapie_enbpi.fit(X_train, y_train)\n\ny_pred_pfit = np.zeros(y_pred_npfit.shape)\ny_pis_pfit = np.zeros(y_pis_npfit.shape)\nconformity_scores_pfit = []\nlower_quantiles_pfit = []\nhigher_quantiles_pfit = []\ny_pred_pfit[:gap], y_pis_pfit[:gap, :, :] = mapie_enbpi.predict(\n    X_test.iloc[:gap, :], alpha=alpha, ensemble=True, optimize_beta=True\n)\nfor step in range(gap, len(X_test), gap):\n    mapie_enbpi.partial_fit(\n        X_test.iloc[(step - gap):step, :],\n        y_test.iloc[(step - gap):step],\n    )\n    (\n        y_pred_pfit[step:step + gap],\n        y_pis_pfit[step:step + gap, :, :],\n    ) = mapie_enbpi.predict(\n        X_test.iloc[step:(step + gap), :],\n        alpha=alpha,\n        ensemble=True,\n        optimize_beta=True\n    )\n    conformity_scores_pfit.append(mapie_enbpi.conformity_scores_)\n    lower_quantiles_pfit.append(mapie_enbpi.lower_quantiles_)\n    higher_quantiles_pfit.append(mapie_enbpi.higher_quantiles_)\ncoverage_pfit = regression_coverage_score(\n    y_test, y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0]\n)\nwidth_pfit = regression_mean_width_score(\n    y_pis_pfit[:, 0, 0], y_pis_pfit[:, 1, 0]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Plot estimated prediction intervals on one-step ahead forecast\n\nLet's now compare the prediction intervals estimated by MAPIE with and\nwithout update of the residuals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_preds = [y_pred_npfit, y_pred_pfit]\ny_pis = [y_pis_npfit, y_pis_pfit]\ncoverages = [coverage_npfit, coverage_pfit]\nwidths = [width_npfit, width_pfit]\n\nfig, axs = plt.subplots(\n    nrows=2, ncols=1, figsize=(14, 8), sharey=\"row\", sharex=\"col\"\n)\nfor i, (ax, w) in enumerate(zip(axs, [\"without\", \"with\"])):\n    ax.set_ylabel(\"Hourly demand (GW)\")\n    ax.plot(\n        y_train[int(-len(y_test)/2):],\n        lw=2,\n        label=\"Training data\", c=\"C0\"\n    )\n    ax.plot(y_test, lw=2, label=\"Test data\", c=\"C1\")\n\n    ax.plot(\n        y_test.index, y_preds[i], lw=2, c=\"C2\", label=\"Predictions\"\n    )\n    ax.fill_between(\n        y_test.index,\n        y_pis[i][:, 0, 0],\n        y_pis[i][:, 1, 0],\n        color=\"C2\",\n        alpha=0.2,\n        label=\"Prediction intervals\",\n    )\n    title = f\"EnbPI, {w} update of residuals. \"\n    title += f\"Coverage:{coverages[i]:.3f} and Width:{widths[i]:.3f}\"\n    ax.set_title(title)\n    ax.legend()\nfig.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now compare the coverages obtained by MAPIE with and without update\nof the residuals on a 24-hour rolling window of prediction intervals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "window = 24\nrolling_coverage_pfit, rolling_coverage_npfit = [], []\nfor i in range(window, len(y_test), 1):\n    rolling_coverage_pfit.append(\n        regression_coverage_score(\n            y_test[i-window:i], y_pis_pfit[i-window:i, 0, 0],\n            y_pis_pfit[i-window:i, 1, 0]\n        )\n    )\n    rolling_coverage_npfit.append(\n        regression_coverage_score(\n            y_test[i-window:i], y_pis_npfit[i-window:i, 0, 0],\n            y_pis_npfit[i-window:i, 1, 0]\n        )\n    )\n\nplt.figure(figsize=(10, 5))\nplt.ylabel(f\"Rolling coverage [{window} hours]\")\nplt.plot(\n    y_test[window:].index,\n    rolling_coverage_npfit,\n    label=\"Without update of residuals\"\n)\nplt.plot(\n    y_test[window:].index,\n    rolling_coverage_pfit,\n    label=\"With update of residuals\"\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training data do not contain a change point, hence the base model cannot\nanticipate it.\nWithout update of the residuals, the prediction intervals are built upon the\ndistribution of the residuals of the training set.\nTherefore they do not cover the true observations after the change point,\nleading to a sudden decrease of the coverage.\nHowever, the partial update of the residuals allows the method to capture the\nincrease of uncertainties of the model predictions.\nOne can notice that the uncertainty's explosion happens about one day late.\nThis is because enough new residuals are needed to change the quantiles\nobtained from the residuals distribution.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}